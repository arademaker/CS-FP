#+AUTHOR: bruno cuconato
#+DATE: 2017-11-03
#+EMAIL: bcclaro+csfp@gmail.com
#+TITLE: The Composition of Meaning in Natural Language

* Recap
  In the last chapter we translated natural language sentences into
  logical forms and then evaluated these logical forms with respect to
  a model. We used Haskell as a lambda calculus engine for this
  construction procedure. This worked well, but it has the
  disadvantage that the Haskell interpreter hides the details of how
  expressions of typed logic get simplified. In this chapter, we will
  demonstrate that the composition of meaning can be carried out
  without a logical form language as intermediate representation
  level. We will show how models of predicate logic can be ex- tended
  to models of typed logic, and how typed logic yields appropriate
  meaning representations for the lexical items of natural language
  fragments and provides the means for building up meanings in a
  compositional way. But before all that, we must focus on
  quantifiers.

* rules of the game
  An invitation to translate English sentences from an example
  fragment of natural language given by some set of grammar rules into
  some logic – predicate logic, or typed logic – presupposes two
  things: 
  - that you grasp the meanings of the formulas of the representation
    language;
  - that you understand the meanings of the English sentences.

  Knowledge of the first kind can be made fully explicit, but because
  of this presupposed under- standing of the original message, such
  translations cannot count as explications of the concept of meaning
  for natural language.

  As we have seen in Section 6.2 (Predicate Logic as Representation
  Language), it is possible to make knowledge of the meaning of a
  fragment of natural language fully explicit. The procedure does not
  presuppose knowledge of the meaning of complete natural language
  sentences. Instead, it specifies how sentence meanings are derived
  from the meanings of smaller building blocks. (The meaning of these
  smallest building blocks is taken as given.)

  In our example, these links are made by the interpretation
  instructions for relation and function symbols in a predicate
  logical model. *Computational semantics* has little or nothing to
  say about the interpretation of semantic atoms. It has rather a lot
  to say, however, about the process of composing complex meanings in
  a systematic way out of the meanings of components.

  - the principle of compositionality :: The meaning of an expression
       is a function of the meanings of its immediate syntactic
       components and their syntactic mode of composition.

* quantification
** Aristotle
  An important goal in computational semantics for natural language is
  to provide an account of the /process of drawing inferences/ in
  natural language.

  Aristotle’s theory of quantification has two logical defects:
  - Quantifier combinations are not treated; only one quantifier per
    sentence is allowed.
  - /Non-standard/ quantifiers such as most, half of, at least five,
    etc, are not covered.
  A minor additional flaw is the assumption of existential
  presupposition.

** Frege
  The first of these defects was removed by Frege. It is based on the
  introduction of individual variables bound by the quantifiers ∀ and
  ∃.  Quantifiers with their associated variables can combine with
  arbitrarily complex predicate logical formulae to form new predicate
  logical formulae, so a formula may contain an arbitrary number of
  quantifiers.

  Using standard quantifiers and equality it is also possible to
  express numerical constraints like At least two A are B:
  : ∃x∃y(x ≠ y ∧ Ax ∧ Ay ∧ Bx ∧ By).
  It is not difficult to see that all disjunctions and conjunctions of
  quantifiers of the forms at least n and at most m can be expressed
  in terms of standard quantifiers and equality.

  To illustrate the claim that first-order logic has no difficulty
  with quantifier combinations, consider the translation:
  : Every prince sang a ballad.
  : ∀x(Prince x → ∃y(Ballad y ∧ Sing x y)).
  Observe that the translation does not contain phrases corresponding
  to the noun phrases every prince or a ballad. Given a natural
  language sentence and its translation into first-order logic, it is
  impossible to pinpoint the sub-expression in the translation that
  gives the meaning of a particular noun phrase in the original. In
  the translation into first-order logic, the noun phrases have been
  syntactically eliminated.

  #+BEGIN_EXAMPLE
  A ballad was sung by every prince.
  ∃y(Ballad y ∧ ∀x(Prince x → Sing x y))
  ∀x(Prince x → ∃y(Ballad y ∧ Sing x y))  
  #+END_EXAMPLE

  The fact that both translations are appropriate for shows that the
  sentence is ambiguous. It also shows that translating into
  first-order logic can be used to disambiguate natural language
  sentences.

  when first-order predicate logic was still considered as the one and
  only tool for semantic analysis, quantified noun phrases were
  commonly regarded as systematically misleading expressions.  Their
  natural language syntax did not correspond to their logic, for in
  natural language they were separate constituents, but they
  evaporated during the process of translation into first-order logic.

  Fregean quantifiers still have problems:
  - finding logical representation languages permitting the
    preservation of noun phrases as separate constituents;
  - finding procedures for translating from natural language to
    logical representations that are not ad hoc; -- what is ad-hoc
    about FOL?
  - finding ways to treat non-standard quantifiers such as most,
    preferably in a uniform framework with standard quantifiers.

** The relational view of quantifiers
   a quantifier is viewed as a two-place relation on the power set of
   a domain of discourse (or universe) =E= satisfying certain
   requirements. (You can read =E= as mnemonic for the set of
   entities, for example.) The power set of a set =E=, notation
   =P(E)=, is the set of all subsets of =E=. A two-place relation on
   =P(E)= is a set of pairs of subsets of =E=. The relational
   perspective on quantification is implicit in Montague grammar.

   Below it will be shown that the relational view can be used to
   remedy the defects of both the Aristotelian and the Fregean
   theory. a representation language with generalized quantifier
   expressions (ex- pressions denoting two-place relations between
   sets) and a notation for lambda abstraction is eminently suited for
   the compositional analysis of natural language sentences with
   quantified noun phrases.

   #+CAPTION: analysis of =Every princess laughed.=
   | expression             | translation                  | type                        |
   |------------------------+------------------------------+-----------------------------|
   | every                  | *every*                      | (e -> t) -> ((e -> t) -> t) |
   | princess               | /Princess/                   | (e -> t)                    |
   | every princess         | *every* /Princess/           | (e -> t) -> t               |
   | laughed                | /Laugh/                      | (e -> t)                    |
   | every princess laughed | (*every* /Princess/) /Laugh/ | t                           |
   |------------------------+------------------------------+-----------------------------|

   The determiner every translates into an expression every denoting a
   function from properties to a function from properties to truth
   values. More precisely, every denotes the function mapping a
   property P to the characteristic function of the set of all
   properties having P as a subset.

   This expression yields true in case the property of being a
   princess is included in the property of laughing, false otherwise.

   To see how quantifier combinations are dealt with compositionally:
   : Every mermaid hummed a song.
   The trick is finding the right translation for the transitive verb. This turns out to
   be the lambda expression =λXλy -> X (λz -> Hum y z)=, where X is a variable
   over noun phrase type expressions.
   #+BEGIN_VERSE
   (λXλy -> X (λz -> /Hum/ y z))(*a* /Song/).
   λy -> (*a* /Song/)(λz -> /Hum/ y z).
   #+END_VERSE
   *a* denotes the function which maps every property P to (the
   characteristic function of) the set of all properties having a
   non-empty overlap with P. Feeding as argument to the expression
   *every* /Mermaid/:
   #+BEGIN_VERSE
   (*every* /Mermaid/)(λy -> ((*a* /Song/)(λz -> /Hum/ y z))).
   #+END_VERSE
   #+CAPTION: analysis of =Every mermaid hummed a song.=
   [[./media/every-mermaid.png]]

** Conditions on Quantifier Relations
   we view quantifiers as two-place relations on the power set of some
   domain E, satisfying certain requirements. which ones?
   
   =All dwarfs work= is true in a given model if and only if the
   relation of inclusion holds between the set of dwarfs in the model
   and the set of workers in the model.

   Abstracting from the domain of discourse, we can say that
   determiner interpretations (henceforth simply called determiners)
   pick out binary relations on sets of individuals, on arbitrary
   domains of discourse E. The notation is D_{E}AB. We call A the
   restriction of the quantifier and B its body. the NP =all dwarfs=
   is the restriction of the determiner all, and the VP =work= is the
   body.
   
   #+CAPTION: Interpretation of D_{E}AB as a relation between sets A and B
   [[./media/deab.png]]

   Not all two-place relations on sets of individuals are quantifier
   relations. The first two requirements that quantifiers must meet
   are general requirements for de- notations of determiners:
   extension and conservativity, which will be abbreviated as EXT and
   CONSE, respectively.
   
*** EXT
   : For all A, B ⊆ E ⊆ E' : DᴇAB ⇔ Dᴇ'AB.
   A relation observing EXT is stable under growth of the
   universe. So, given sets A and B, only the objects in the minimal
   universe A ∪ B matter.  An example is all: to determine the truth
   of All dwarfs work, we only need the intersection of the set of
   dwarfs and the set of workers. It does not matter at all how many
   or which kind of entities are contained in the rest of the
   domain. But not all natural language determiners do satisfy EXT. An
   example of a determiner that does not is /many/ in the sense of
   /relatively many/.

*** CONSE
    : For all A, B ⊆ E: DᴇAB ⇔ DᴇA(A ∩ B).
    This property expresses that the first argument of a determiner
    relation (the inter- pretation of the noun) plays a crucial role:
    everything outside the extension of the first argument is
    irrelevant. /Some/ is an example for a conservative determiner: to
    determine the truth of =Some dwarfs work=, we only need to check
    whether the set of dwarfs contains workers – nothing outside the
    set of dwarfs will have any effect on the truth or falsity of the
    sentence.
    
    One example that does not satisfy CONSE is only in the following
    sentence.
    : Only dwarfs sing during work.
    This example is true in a situation where all singing workers are
    dwarfs. Starting out from a situation like this, and adding some
    non-dwarfs to the singing workers will it false. This shows
    non-conservativity.

    All is still well if it can be argued that noun phrases starting
    with /only/, /mostly/, or /mainly/ (two other sources of
    non-conservativity) are exceptional syntactically, in the sense
    that these noun phrase prefixes are not really determiners. In the
    case of only, it could be argued that only dwarfs has structure
    : [NP [MOD only][NP dwarfs]]
    , with only not a determiner but a noun phrase modifier, just as
    in:
    : Only Bombur sings during work.

    However this may be, separating out the determiners satisfying
    CONSE and EXT is important, for the two conditions taken together
    ensure that the truth of =D AB= depends only on =A − B= and =A ∩
    B=. (Thus, the combined effect of EXT and CONSE boils down to
    limiting the domain of discourse relevant for the truth or falsity
    of D E AB to two sets: the set of things which are A but not B,
    and the set of things which are both A and B).

*** ISOM
    Next, the relational perspective suggests a very natural way of
    distinguishing between expressions of quantity and other
    relations. Quantifier relations satisfy the following condition of
    isomorphy, formulated in terms of bijections.
    : If f is a bijection from E to E', then DᴇAB ⇒ Dᴇ'f[A]f[B].
    Here f [A], the image of A under f , is the set of all things
    which are f-values of things in A. ISOM expresses that only the
    cardinalities (numbers of elements) of the sets A and B matter,
    for the image of a set under a bijection is a set with the same
    number of elements as the original set. If D satisfies EXT, CONS,
    and ISOM, it turns out that the truth of D AB depends only on the
    cardinal numbers =|A-B|= and =|A∩B|=

    A quantifier simply is a relation Q satisfying EXT, CONS, and
    ISOM.
    #+CAPTION: The Combined Effect of EXT, CONS, ISOM.
    [[./media/ext-conse-isom.png]]
    
    examples:
    - =All A are B= is true if and only if the number of things which
      are A and not B is 0.
    - =Some A is B= is true if and only if the number of things that are
      both A and B is at least 1.
    - =Most A are B= is true if and only if the number of things that
      are both A and B exceeds the number of things that are A and not
      B.

** numerical trees
   We now turn to a way to characterize quantifiers Q AB according to
   the two numbers =|A − B|= and =|A ∩ B|=. We will then use this
   characterization for the logical representation of quantifiers.

   #+INCLUDE: "./TCOM.hs" src haskell :lines "7-14"
   #+INCLUDE: "./TCOM.hs" src haskell :lines "16-24"

** Logical Representations for Quantifiers
   The pairs of cardinals that characterize a quantifier Q AB can be
   used for representation purposes. Every quantifier is defined by
   means of an arithmetical expression in two variables m and n, where
   m is the number of elements in A − B, n the number of elements in A
   ∩ B. Logical forms for quantified expressions can exploit this fact
   (=↦= :: /translates as/):
   - at least two ↦ λm,n -> n ≥ 2.
   - all ↦ λm,n -> m = 0.
   - no ↦ λm,n -> n = 0.

   Logical operations on quantifiers can now be handled
   compositionally, by performing the corresponding logical
   operations on the arithmetical expressions:
   - If /Q ↦ E/, then /not Q ↦ λm,n -> ¬ (Emn)/.
   - If /Q_1 ↦ E_1/ and /Q_2 ↦ E_2/ , then /Q_1/ and /Q_2 ↦ λm,n ->
     ((E_1 mn) ∧ (E_2 mn))/ and /[Q_1 or Q_2 ] ↦ λm,n -> ((E_1 mn) ∨
     (E_2 mn))/.

** Relational Properties
   As quantifiers are relations, we can study their relational
   properties and the way in which these properties are reflected in
   the tree patterns. For example, a quantifier Q is reflexive if and
   only if =∀X Q XX=. E.g. the quantifiers all and some are reflexive,
   the quantifiers no and not all are not.
   
   One can now study questions about tree patterns such as the
   following. If Q is reflexive, what will its tree pattern be like?
   Can it be shown that every quantifier with this tree pattern is
   reflexive? If some quantifier Q has a tree pattern with an outer
   north east diagonal consisting of minus signs, which relational
   property of Q does this reflect?

   A relational property with linguistic interest is symmetry. A
   quantifier Q is symmetric if and only if =∀X∀Y Q XY ⇔ Q Y X=.  The
   linguistic interest of this class lies in the fact that the
   symmetric quantifiers are precisely the class of quantifiers which
   can occur at the Q position in /there/-existential sentences
   (sentences of the form =There are Q=, e.g., =There are some...=).

   its tree pattern is ...

   Another example of a relational property of quantifiers with
   linguistic interest (to be illustrated below) is upward
   right-monotonicity in the second argument place:
   - MON↑ :: If Q AB and B ⊆ B', then Q AB'.
   This means that the truth or falsity of Q AB does not change if the
   set B is ex- tended. Examples of quantifiers that upward
   right-monotone are all, some, and at least five.

   its tree pattern is ...

   A quantifier relation is downward right-monotone in the second argument if the
   following holds:
   - MON↓ :: If Q AB and B' ⊆ B, then Q AB'.
   I.e. the truth or falsity of Q AB is not affected by a reduction of
   the set B. Examples are not all and no.

   its tree pattern is ...

   An example for a quantifier that satisfies neither MON↑ nor MON↓ is
   an even number of . (You can see this by inspecting the tree
   pattern.)

   - ↑MON :: If Q AB and A ⊆ A', then Q A' B.
   - ↓MON :: If Q AB and A' ⊆ A, then Q A' B.
   Examples of ↑MON determiners are some and not all. All and no are
   ↓MON determiners.

** Quantifiers, [[http://www.inf.ed.ac.uk/teaching/courses/inf1/cl/notes/Comp1.pdf][Automata]], and Definability
#+BEGIN_QUOTE


- c7e11 :: Construct finite state machines for computing:
  - /at least two/
#+BEGIN_SRC dot :file ./media/at-least-2.png :cmdline -Kdot -Tpng :cache yes
digraph atLeast2 {
        rankdir=LR;
        node [label="",shape=circle];
        start [style=invis];
        a [shape=doublecircle];
        s0;
        s1;
        start -> s0;
        s0 -> s0 [label="0"];
        s0 -> s1 [label="1"];
        s1 -> s1 [label="0"];
        s1 -> a [label="1"];
        a -> a [label="1"];
        a -> a [label="0"];
        }  
#+END_SRC

#+RESULTS:
[[file:./media/at-least-2.png]]

  - /at most five/
#+BEGIN_SRC dot :file ./media/at-most-five.png :cmdline -Kdot -Tpng :cache yes
  digraph atMost5 {
          rankdir=LR;
          node [label="",shape=doublecircle];
          start [style=invis];
          na [shape=circle];
          s0;
          s1;
          s2;
          s3;
          s4;
          start -> s0;
          s0 -> s0 [label="0"];
          s0 -> s1 [label="1"];
          s1 -> s1 [label="0"];
          s1 -> s2 [label="1"];
          s2 -> s2 [label="0"];
          s2 -> s3 [label="1"];
          s3 -> s3 [label="0"];
          s3 -> s4 [label="1"];
          s4 -> s4 [label="0"];
          s4 -> na [label="1"];
          na -> na [label="1"];
          na -> na [label="0"];
          }
#+END_SRC

#+RESULTS:
[[file:./media/at-most-five.png]]

  - /between three and seven/
#+BEGIN_SRC dot :file ./media/bet-3-and-7.png :cmdline -Kdot -Tpng :cache yes
  digraph between3And7 {
          rankdir=LR;
          node [label="",shape=circle];
          start [style=invis];
          na;
          s0;
          s1;
          s2;
          node [label="",shape=doublecircle];
          s3;
          s4;
          s5;
          start -> s0;
          s0 -> s0 [label="0"];
          s0 -> s1 [label="1"];
          s1 -> s1 [label="0"];
          s1 -> s2 [label="1"];
          s2 -> s2 [label="0"];
          s2 -> s3 [label="1"];
          s3 -> s3 [label="0"];
          s3 -> s4 [label="1"];
          s4 -> s4 [label="0"];
          s4 -> s5 [label="1"];
          s5 -> s5 [label="0"];
          s5 -> na [label="1"];
          na -> na[label="1"];
          na -> na[label="0"];
          }
#+END_SRC

#+RESULTS:
[[file:./media/bet-3-and-7.png]]
#+END_QUOTE

Call a finite state machine permutation invariant if it has the
following property: if reading a string s will get the machine from
state p to state q, then reading any permutation of s will also get
the machine from state p to state q.

A finite state machine is acyclic if the machine does never return to
a given state once it has left that state (in other words: 1-cycles
are allowed, but all other cycles are out). An example of a quantifier
that can be computed by a cyclic finite state machine but not by an
acyclic one is an even number of.

A quantifier is called first-order definable if it is definable in
terms of the Fregean quantifiers ∀ and ∃, equality, and the two
predicates for the restriction and the body of the quantifier. The
question of first-order definability is relevant for the seman- tics
of natural language, because the suitability of logical representation
languages for given natural language fragments depends on it.

#+BEGIN_QUOTE
The first-order definable quantifiers are exactly those that can be
computed by an acyclic permutation-invariant finite state machine
#+END_QUOTE

It follows from this that an even number of is not first-order
definable (a cyclic automaton is needed for its computation), nor are
quantifiers like half and most, which cannot be computed on a finite
state machine at all (a memory stack is needed to ‘remember’ the
numbers of elements in A − B and A ∩ B).

#+BEGIN_QUOTE

- c7e12 :: The automata perspective can be exploited to give an
           account of semi-quantifiers involving ordinals:

  - /Every tenth page of a fairy tale is boring./
#+BEGIN_SRC dot :file ./media/every-tenth.png :cmdline -Kdot -Tpng
  digraph every10 {
           rankdir=LR;
           node [label="",shape=circle];
           start [style=invis];
           s0;
           s1;
           s2;
           s3;
           s4;
           s5;
           s6;
           s7;
           s8;
           s9;
           a [shape=doublecircle];
           start -> s0;
           s0 -> s1 -> s2 -> s3 -> s4 -> s5 -> s6 -> s7 -> s8 -> s9 -> a;
           a -> a;
           }
#+END_SRC

#+RESULTS:
[[file:./media/every-tenth.png]]

  - /The first ten pages of a fairy tale are boring./
#+BEGIN_SRC dot :file ./media/first-ten.png :cmdline -Kdot -Tpng
  digraph first10 {
           rankdir=LR;
           node [label="",shape=doublecircle];
           start [style=invis];
           s0;
           s1;
           s2;
           s3;
           s4;
           s5;
           s6;
           s7;
           s8;
           s9;
           na [shape=circle];
           start -> s0 -> s1 -> s2 -> s3 -> s4 -> s5 -> s6 -> s7 -> s8 -> s9 -> na;
           na -> na;
           }
#+END_SRC

#+RESULTS:
[[file:./media/first-ten.png]]
#+END_QUOTE

* The Language of Typed Logic and Its Semantics
  Assume that we have constants and variables available for all types
  in the type hierarchy. Then the language of typed logic over these
  is defined as follows:
  #+BEGIN_VERSE
type ::= e | t | (type -> type)
expression ::= constant_type
             | variable_type
             | (\ variable_type_1 -> expression_type_2)_(type_1 -> type_2)
             | (expression_{type_1 -> type_2} expression_type_1)_type_2
  #+END_VERSE

  #+BEGIN_QUOTE
  - c7e13 :: Assume constant A has type e -> t and constant B has type 
  (e -> t) -> t.  Variable x has type e, variable Y has type e ->
  t. Which of the following expressions are well-typed?
    - (\x -> (A x)) :: yes: =e -> t=.
    - (B (\x -> (A x))) :: yes: =t=.
    - (\Y -> (Y (\x -> (A x)))) :: yes: =((e -> t) -> x) -> x=, where
         x is some type.
    - (\Y -> (B Y)) :: yes: =(e -> t) -> t=.
  #+END_QUOTE

  A model M for typed logic consists of a domain D e together with an
  interpretation function I which maps every constant of the language
  to a function of the appropriate type in the domain hierarchy based
  on D_e. A variable assignment g for typed logic maps every variable
  of the language to a function of the appropriate type in the domain
  hierarchy. The semantics for the language is given by defining a
  function []^M_g which maps every expression of the language to a
  function of the appropriate type.

  - [ *constant* ]^M_g = I(*constant*)
  - [ *variable* ]^M_g = g(*variable*)
  - [(\ v_t1 -> E_t2)]^Mg = /h/, where h:D_t1 -> D_t2 is the function
    given by \d -> [E]^M_{g[v:=d]}
  - [(E_1 E_2)]^M_g = [E_{1}]^M_g([E_{2}]^M_g)

  In fact, the logical constants of predicate logic can be viewed as
  constants of typed logic, as follows: ¬ is a constant of type t → t
  with the following interpretation.
  - [¬] = h, where h is the function in t → t which maps 0 to 1 and
    vice versa;
  As we have seen already, ∧ and ∨ are constants of type t → t → t
  with the following interpretations.
  - [∧] = h, where h is the function in t → t → t which maps 1 to {(1,
    1), (0, 0)} and 0 to {(1, 0), (0, 0)};
  - [∨] = h, where h is the function in t → t → t which maps 1 to {(1,
    1), (0, 1)} and 0 to {(1, 1), (0, 0)};
  Note that {(1, 1), (0, 0)} is the identity function on {0, 1}.
  - c7e15 :: Give the interpretation of the material implication
             constant =->= in typed logic.  
  =->= has type T = =t -> t -> t=, and interpretation function [->] =
  h, where h is the function in T which maps 1 to {(1,1), (0,0)} and 0
  to {(1,1), (0,1)};
  - c7e16 :: Give the interpretation of the material equivalence
             constant =<->= in typed logic.
  =<->= has type T = =t -> t -> t=, and interpretation function [<->]
  = h, where h is the function in T which maps 1 to {(1,1), (0,0)} and
  0 to {(1,0), (0,1)};

  The quantifiers ∃ and ∀ are constants of type (e → t) → t, with the
  following interpretations.
  - [∀] = h, where h is the function in (e → t) → t which maps the function that
  characterizes D_e to 1 and every other characteristic function to 0;
  - [∃], where h is the function in (e → t) → t which maps the function that
  characterizes ∅ to 0 and every other characteristic function to 1.

  It is possible to add constants for quantification over different
  types. E.g. to express second-order quantification
  (i.e. quantification over properties of things), one would need
  quantifier constants of type ((e → t) → t) → t.

# exercise 7.17 - doubt?
  
  We will assume that for every type τ built from e and t we have a
  constant i_{τ -> τ -> t} available to express the identity of two
  objects of type τ .  Then i_{τ -> τ -> t} denotes identity of
  individual objects, and it is convenient to abbreviate ((i b) a),
  for a, b of type e, as a = b.

  - c7e18 :: Write out i_{t -> t -> t} . Which two-place connective
             does this constant express?  
  it expresses =<->=.

  #+CAPTION: expressions in typed logic and their counterparts in predicate logic.
  | Typed logic                       | Predicate logic |
  |-----------------------------------+-----------------|
  | (¬ (P x))                         | ¬Px             |
  | ((∧ (P x)) (Q y))                 | Px∧Qy           |
  | (∀ (λx -> (P x)))                 | ∀xPx            |
  | (∀ (λx -> (∃ (λy -> ((R x) y))))) | ∀x∃yRxy         |
  |-----------------------------------+-----------------|

  It is possible to reduce the difference in syntactic appearance
  between ordinary predicate logic and typed logic by means of some
  abbreviation conventions. Occasionally we will write (λx -> (λy ->
  E)) as λxy -> E. E)) as λxy -> E, and similarly for three or more
  successive lambda abstractions. Also, an expression of the form (((E
  a) b) c) can be written as Eabc. Finally, it is convenient to write
  ((∧ E 1 ) E 2 ) as E 1 ∧ E 2 and similarly for ∨, → and ↔. We will
  also omit outermost parentheses.

  Note that although (((E a) b) c) may be written as Eabc, this is not
  the same as E(a, b, c). The difference is that in Eabc, the
  arguments of the function E are consumed one by one, while in E(a,
  b, c), the function E takes a single argument which is a triple.

  - c7e19 :: Exercise 7.19 Assume Give(x, y, z) means that x gives y
             to z. Use this to find a typed logic expression for
             ‘receiving something from someone’.
  \x -> ∃y.∃z.Give(y,z,x).

  Suppose we want to translate Siegfried gave Kriemhild the ring,
  which has syntactic structure in a compositional way.
  : [ S [ NP Siegfried ][ VP [ TV [ DTV gave ] [ NP Kriemhild ]][ NP the ring ]]]
  So we want to use λzyx -> Give(x, y, z) as translation for
  /gave/. In curried notation this would correspond to λzyx -> Give x
  y z or λzyx -> (((Give x) y) z). In the latter, however, the order
  in which Give takes its arguments does not reflect the argument
  order that we read off the syntactic structure.

  That’s why in the following chapter we will use constants with a
  different argument order, such that we can construct the meaning
  λzyx -> (((Give z) y) x), which reflects that Give is first applied
  to the indirect object meaning, then to the direct object meaning,
  and then to the subject meaning.

* Reducing Expressions of Typed Logic
  Translating all the combinations of phrases as function argument
  combination in the sentence above, we arrive at the following
  translation of the whole sentence: 
  : (((λzyx -> Give(x, y, z) c) b) a).
  To reduce this expression from the previous section to its simplest
  form, three steps of so-called β-conversion are needed.

  During β-conversion of an expression consisting of a function
  expression =λv -> E= followed by an argument expression A, basically
  the following happens (we will state a condition shortly). The
  prefix =λv ->= is removed from the function expression =λv -> E=,
  leaving E, and next the argument expression A is substituted in E
  for all free occurrences of v. The free occurrences of v in E are
  precisely the occurrences which were bound by λv in =λv -> E=.

  In some cases, the substitution process described above cannot be
  applied without further ado, because it will result in unintended
  capturing of variables within the argument expression A.
  : ((λx -> (λy -> ((R y) x))) y).
  In this expression, y is bound in the functional part (λx → (λy →
  ((R y) x))) but free in the argument part y. Reducing it by
  β-conversion according to the recipe given above would result in (λy
  -> ((R y) y)), with capture of the argument y at the place where it
  is substituted for x. This problem can be avoided by performing
  β-conversion on an alphabetic variant of the original expression,
  say on:
  : ((λx → (λz → ((R z) x))) y).
  
  Another example where switching to an alphabetic variant (also
  called α-conversion) is necessary before β-conversion to prevent
  unintended capture of free variables is the expression:
  : ((λp → ∀x((P x) ↔ p))(Q x)).
  p is a variable of type t and x one of type e. Variable x is bound
  inside the functional part (λp → ∀x((P x) ↔ p)) but free in the
  argument part (Q x).  Substituting (Q x) for p in the function
  expression would cause x to be captured, with failure to preserve
  the original meaning. Again, the problem is avoided if β-conversion
  is performed on an alphabetic variant of the original expression:
  : ((λp → ∀z((P z) ↔ p))(Q x))
  Performing β-reduction on it yields ∀z((P x) ↔ (Q x)), with the
  argument of B still free, as it should be.

  To state all this in a formally precise way, we start with a
  definition of "variable v is free in expression E". v is free in E
  if the following holds (we use ≈ for the relation of being
  syntactically identical, i.e. for being the same expression):
  - v ≈ E;
  - E ≈ (E_1 E_2 ), and v is free in E_1 or v is free in E_2;
  - E ≈ (λx → E_1 ), and v ≉ x, and v is free in E_1.

  - c7e20 :: Which occurrences of x are free?
    - (λx → (P x)): none.
    - ((λx → (P x)) x): the last one.
    - (λx → ((R x) x)): none.
    - ((λx → ((R x) x))x): the last one.
    - ((λy → ((R x) y))x): all of them.

  - c7e21 :: Same question for ((λy → ∃x((R x) y)) x), where one
             should bear in mind that ∃x((R x) y) is shorthand for (∃
             (λx → ((R x) y))).
  the last one.

  Next, we give the definition of substitution of s for free
  occurrences of v in E, with notation E[v := s]:
  - If E ≈ v, then E[v := s] ≈ s, if E ≈ x ≉ v (i.e. E is a variable
    different from v), then E[v := s] ≈ x, if E ≈ c (i.e. E is a
    constant, and therefore different from v), then E[v := s] ≈ c.
  - If E ≈ (E_1 E_2 ), then E[v := s] ≈ (E_{1}[v := s] E_{2}[v := s]).
  - If E ≈ (λx → E_1 ), then:
    - if v ≈ x, then E[v := s] ≈ E,
    - if v ≉ x, then there are two cases:
      - if x is not free in s or v is not free in E, then E[v := s] ≈
        (λx → E_1 [v := s]);
      - if x is free in s and v is free in E, then E[v := s] ≈ (λy →
        E_1 [x := y][v := s]), for some y which is not free in s and
        not free in E_1.

  we need the last bullet because:
  : (λy → (P x))[x := y]
  consider applying the second last bullet instead of the last.
  
  Finally, here is the definition of reduction, which comes in three
  flavours: β-reduction, α-reduction, and η-reduction, for which we
  use arrows -α->, -β->, and -η->.

  - Beta reduction :: ~((λv → E) s) -β-> E[v := s]~
  Condition: v and s are of the same type (otherwise the expression to
  be reduced is not well-typed).
  - Alpha reduction :: ~(λv → E) -α-> (λx → E[v := x])~
  Conditions: v and x are of the same type, and x is not free in E.
  - Eta reduction: =((λv → E) v) -η-> E=
    
  The ‘real work’ takes place during β-reduction. The α-reduction rule
  serves only to state in an explicit fashion that lambda calculations
  are insensitive to switches to alphabetic variants. The η-reduction
  rule makes a principle explicit that we have used implicitly all the
  time: if (P j) expresses that John is present, then both P and (λx →
  (P x)) express the property of being present. This is so η because
  ((λx → (P x)) x) -η-> (P x), so P and (λx → (P x)) give the same
  result when applied to argument x, i.e. they express the same
  function.

  Applying β-reduction to
  : (((λzyx → Give(x, y, z) c) b) a) -β-> Give(a, b, c)

  to be fully precise we have to state explicitly that expressions can
  be reduced 'in context':

# not sure I got this?
  | E -β-> E'         |
  |-------------------|
  | (F E) -β-> (F E') |

  | E -β-> E'         |
  |-------------------|
  | (E F) -β-> (E' F) |

  | E -β-> E'                 |
  |---------------------------|
  | (\v -> E) -β-> (\v -> E') |

  Here F is assumed to have the appropriate type, of course. These
  principles allow β-reductions at arbitrary depth within expressions.

  - c7e22 :: Reduce the following expressions to their simplest forms.
    - ((λY → (λx → (Y x))) P ) ≈ (\x -> (P x))
    - (((λY → (λx → (Y x))) P ) y) ≈ (P y)
    - ((λP → (λQ → ∃x(P x ∧ Qx))) A) ≈ (\Q -> ∃x(A x ∧ Q x))
    - (((λP → (λQ → ∃x(P x ∧ Qx))) A) B) ≈ ∃x(A x ∧ B x)
    - ((λP → (λQ -> ∀x(P x → Qx)))(λy → ((λx → R(x, y)) j))) ≈ 
      ((λP → (λQ -> ∀x(P x → Qx)))(λy → R(j, y))) ≈
      (λQ -> ∀x(((λy → R(j, y)) x) → Qx)) ≈
      (\Q -> ∀x(R(j, x) → Q x))

  we write E_1 ->> "E_1 reduces in a number of α, β, η steps to E_2 ."

  - Confluence property (or: Church-Rosser property) :: For all
       expressions E, E_1 , E_2 of typed logic: if E ->> E_1 and E ->>
       E_2 , then there is an expression F with E_1 ->> F and E_2 ->>
       F .

  An expression of the form ((λv → E) s) is called a *β-redex* (for:
  β-reducible expression). E[v := s] is called the *contractum* of
  ((λv → E) s). An expression that does not contain any redexes is
  called a *normal form*.

  - Normal form property :: Every expression of typed logic can be
       reduced to a normal form.

  Combining the confluence property and the normal form property we
  get that the normal forms of an expression E are identical modulo
  α-conversion. That is to say, all normal forms of E are alphabetic
  variants of one another.

  The normal form property holds thanks to the restrictions imposed by
  the typing discipline. Untyped lambda calculus lacks this
  property. In untyped lambda calculus it is allowed to apply
  expressions to themselves. In typed lambda calculus this is
  forbidden, because (X X) cannot be consistently typed.

  - c7e23 :: In untyped lambda calculus, expressions like (λx → (x x))
             are well-formed. Show that ((λx -> (x x))(λx → (x x)))
             does not have a normal form.
  ((λx -> (x x))(λx → (x x))) -α-> ((\y -> (y y))(\x -> (x x))) -β->
  ((\x -> (x x))(\x -> (x x))), which takes us back to where we were;
  therefore, we can not find a normal form for this expression.

* Typed Meanings for Natural Language
  Now we are ready for another exercise in composition of meaning for
  natural lan- guage. We will illustrate the theory with our fragment
  from before. Every syntax rule has a semantic counterpart to specify
  how the meaning representation of the whole is built from the
  meaning representations of the components. [X] is used as notation
  for the meaning of X.  

  Sentences are interpreted as function application of the NP meaning
  to the VP meaning:
  : S −→ NP VP 
  : [S] −→ ([NP] [VP])

  NPs are interpreted as generalized quantifiers, or as determiner
  meanings applied to common noun meanings:

  | NP --> /Snow White/ | [NP] --> \P -> (P s)   |
  | NP --> /Alice/      | [NP] --> \P -> (P a)   |
  | ...                 | ...                    |
  | NP --> DET CN       | [NP] --> ([DET] [CN])  |
  | NP --> DET RCN      | [NP] --> ([DET] [RCN]) |
 
  *(see book pp. 173--)*

  example: =[ S [ NP Alice ][ VP [ TV admired ][ NP Dorothy ]]]=
  : ([Alice] (λu → [Dorothy] (λv → (((λx → (λy → ((Admire x) y))) v) u))))
  : ->> ((Admire d) a).

  - c7e24 :: Give the compositional translation for /Snow White helped
             some dwarf/, and reduce it to normal form.
  : [S [NP Snow White] [VP [TV helped] [NP [DET some] [CN Dwarf]]]]
  : ([Snow White] ((\u -> ([NP [DET some] [CN Dwarf]] ((\v -> (\xy -> ((Help x) y))) v))) u))
  : ([Snow White] ((\u -> ([NP [DET some] [CN Dward]] ...
  : ∃x. (Dwarf x ∧ Helped s x)

* Implementing Semantic Interpretation
  We will proceed by defining for every syntactic category an
  interpretation function of the appropriate type, using Entity for e
  and Bool for t. The interpretation of sentences has type Bool, so
  the interpretation function intS gets type Sent -> Bool.

  *check book or code*

* Handling Ambiguity
  If a natural language expression E is ambiguous, i.e. if E has
  several distinct meanings, then, under the assumption that these
  meanings are arrived at in a compositional way, there are three
  possible sources for the ambiguity (combinations are possible, of
  course):
  - The ambiguity is lexical: E contains a word with several distinct
    meanings.  An example is /a splendid ball/.
  - The ambiguity is structural: E can be assigned several distinct
    syntactic structures. Examples are /old [men and women]/ versus
    /[old men] and women/, or: /the boy saw the [girl with the
    binoculars]/ versus /the boy saw [the girl] [with the
    binoculars]/.
  - The ambiguity is derivational: the syntactic structure that E
    exhibits can be derived in more than one way. An example is the
    sentence /Every prince sang some ballad/. It is not structurally
    ambiguous, but in order to account for the ∃ ∀ reading one might
    want to assume that one of the ways in which the structure can be
    derived is by combining /some ballad/ with the incomplete
    expression /every prince sang __/.

** lexical ambiguities
   Lexical ambiguities can be handled in a fragment like ours by means
   of multiplying lexical entries. The two meanings of ball give rise
   to different translations:
   | CN −→ ball  | [CN] --> (\x -> (Ball_1 x)) |
   | CN --> ball | [CN] --> (\x -> (Ball_2 x)) |

   If we use these translations, then lexical ambiguity shows up in
   the fact that /The ball was nice/ will receive two parses, each
   with its own translation.
